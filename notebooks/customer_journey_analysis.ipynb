{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9e8279",
   "metadata": {},
   "source": [
    "# Customer Journey Analysis\n",
    "# This notebook analyzes customer journeys across different products, visualizing patterns in purchasing behavior, demographics, and product adoption sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e994e0",
   "metadata": {},
   "source": [
    "## Import and plot-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use(\"seaborn-v0_8-dark-palette\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcd216",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load all ABT_score files and combine them with appropriate target labels.\n",
    "def load_abt_files():\n",
    "    \"\"\"Load all ABT_score files and combine them with appropriate target labels\"\"\"\n",
    "    # List all ABT_Score/ABT_score files in current directory (case insensitive)\n",
    "    abt_files = list(Path('../data').glob('ABT_[Ss]core_*.csv'))\n",
    "    \n",
    "    # Check if any files were found\n",
    "    if not abt_files:\n",
    "        print(\"No ABT_score_*.csv files found in current directory!\")\n",
    "        print(\"\\nCurrent directory contents:\")\n",
    "        print([f.name for f in Path('../data').glob('*')])\n",
    "        print(\"\\nPlease ensure your ABT_score_*.csv files are in the data directory.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(abt_files)} ABT_score files:\")\n",
    "    for f in abt_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in abt_files:\n",
    "        product = file_path.stem.split('_')[-1]\n",
    "        try:\n",
    "            print(f\"\\nLoading {product} data...\")\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"Successfully loaded {len(df)} rows for {product}\")\n",
    "            df['product_type'] = product\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data frames were successfully loaded. Please check the file format and contents.\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Load the data\n",
    "combined_df = load_abt_files()\n",
    "\n",
    "if combined_df is not None:\n",
    "    # Display basic information about the dataset\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"Total number of records: {len(combined_df)}\")\n",
    "    print(\"\\nProduct distribution:\")\n",
    "    print(combined_df['product_type'].value_counts())\n",
    "else:\n",
    "    print(\"\\nPlease fix the data loading issues before continuing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a55e99",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and preprocess the combined dataset without NaN flag columns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime (invalid -> NaT)\n",
    "    date_columns = [col for col in df.columns if 'Date' in col or 'date' in col]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')  # Coerce invalid dates to NaT\n",
    "    \n",
    "    # Fill numeric NaNs with 0 (assuming 0 indicates absence)\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    \n",
    "    # Convert binary columns to int (original NaN handling)\n",
    "    binary_columns = [col for col in df.columns if col.startswith(('Have_', 'Had_', 'Optout_'))]\n",
    "    for col in binary_columns:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_valid_data(df, column, include_converted_nans=False):\n",
    "    \"\"\"\n",
    "    Get data excluding NaN values for specific column\n",
    "    \n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "    column : str - column name to analyze\n",
    "    include_converted_nans : bool - if True, includes values that were converted from NaN to 0\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with only valid data for the specified column\n",
    "    \"\"\"\n",
    "    if f\"{column}_was_nan\" in df.columns and not include_converted_nans:\n",
    "        return df[~df[f\"{column}_was_nan\"]]\n",
    "    return df\n",
    "\n",
    "def analyze_column_with_nans(df, column):\n",
    "    \"\"\"\n",
    "    Analyze a column's statistics both including and excluding converted NaN values\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalysis for column: {column}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    # All data including converted NaNs\n",
    "    print(\"Including converted NaN values:\")\n",
    "    print(df[column].describe())\n",
    "    \n",
    "    # Only original non-NaN values\n",
    "    if f\"{column}_was_nan\" in df.columns:\n",
    "        valid_data = get_valid_data(df, column)\n",
    "        print(\"\\nExcluding converted NaN values:\")\n",
    "        print(valid_data[column].describe())\n",
    "        \n",
    "        print(f\"\\nTotal values: {len(df)}\")\n",
    "        print(f\"Original NaN values: {df[f'{column}_was_nan'].sum()}\")\n",
    "        print(f\"Valid values: {len(valid_data)}\")\n",
    "    else:\n",
    "        print(\"\\nNo NaN tracking available for this column\")\n",
    "\n",
    "combined_df = preprocess_data(combined_df)\n",
    "\n",
    "# Display sample of preprocessed data\n",
    "print(\"Sample of preprocessed data:\")\n",
    "display(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cec7a",
   "metadata": {},
   "source": [
    "## Customer Journey Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the sequence of products purchased by customers.\n",
    "def analyze_product_sequence(df):\n",
    "    \"\"\"Analyze product sequence using valid dates (NaT excluded)\"\"\"\n",
    "    product_cols = [col for col in df.columns if col.startswith('mFirst_')]\n",
    "    \n",
    "    timeline_data = []\n",
    "    for customer_id in df['sCustomerNaturalKey'].unique():\n",
    "        customer_data = df[df['sCustomerNaturalKey'] == customer_id]\n",
    "        \n",
    "        products = []\n",
    "        for col in product_cols:\n",
    "            product = col.replace('mFirst_', '')\n",
    "            date = customer_data[col].iloc[0]\n",
    "            if pd.notna(date):  # Automatically excludes NaT\n",
    "                products.append({\n",
    "                    'sCustomerNaturalKey': customer_id,\n",
    "                    'product': product,\n",
    "                    'acquisition_date': date\n",
    "                })\n",
    "        \n",
    "        products = sorted(products, key=lambda x: x['acquisition_date'])\n",
    "        timeline_data.extend(products)\n",
    "    \n",
    "    return pd.DataFrame(timeline_data)\n",
    "\n",
    "# Analyze product sequences\n",
    "product_timeline = analyze_product_sequence(combined_df)\n",
    "\n",
    "# Display summary of product sequences\n",
    "print(\"Most common first products:\")\n",
    "display(product_timeline.groupby('sCustomerNaturalKey')\n",
    "        .first()['product']\n",
    "        .value_counts()\n",
    "        .head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e9309",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Journey Sankey Diagram\n",
    "def plot_customer_journey_sankey(df):\n",
    "    \"\"\"Create a Sankey diagram of customer journeys\"\"\"\n",
    "    product_sequence = analyze_product_sequence(df)\n",
    "    \n",
    "    # Group by customer and create product sequences\n",
    "    customer_sequences = product_sequence.groupby('sCustomerNaturalKey').agg(\n",
    "        list\n",
    "    )['product'].value_counts().head(10)  # Top 10 most common sequences\n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node = dict(\n",
    "            pad = 15,\n",
    "            thickness = 20,\n",
    "            line = dict(color = \"black\", width = 0.5),\n",
    "            label = customer_sequences.index,\n",
    "            color = \"blue\"\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = [i for i in range(len(customer_sequences)-1)],\n",
    "            target = [i+1 for i in range(len(customer_sequences)-1)],\n",
    "            value = customer_sequences.values[:-1]\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(title_text=\"Most Common Customer Journey Paths\", \n",
    "                     font_size=10,\n",
    "                     height=600)\n",
    "    fig.show()\n",
    "\n",
    "# Create Sankey diagram\n",
    "plot_customer_journey_sankey(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a782b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic Analysis\n",
    "def plot_demographic_distribution(df):\n",
    "    \"\"\"Plot age distribution for different products (exclude 0 values)\"\"\"\n",
    "    # Filter out 0 ages (original NaNs)\n",
    "    df_valid_age = df[df['Age'] > 0]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    sns.boxplot(x='product_type', y='Age', data=df_valid_age, ax=ax1)\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "    ax1.set_title('Age Distribution by Product (Excluding Missing Values)')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMean age by product (excluding missing values):\")\n",
    "    display(df_valid_age.groupby('product_type')['Age'].mean().sort_values(ascending=False))\n",
    "    \n",
    "plot_demographic_distribution(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Adoption Timeline\n",
    "def plot_product_adoption_timeline(df):\n",
    "    \"\"\"Plot timeline using valid dates only\"\"\"\n",
    "    timeline_data = analyze_product_sequence(df)\n",
    "    \n",
    "    # Filter out any remaining invalid dates (shouldn't be any)\n",
    "    timeline_data = timeline_data[pd.notna(timeline_data['acquisition_date'])]\n",
    "    \n",
    "    fig = px.scatter(timeline_data, \n",
    "                    x='acquisition_date', \n",
    "                    y='product',\n",
    "                    color='product',\n",
    "                    title='Product Adoption Timeline')\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "plot_product_adoption_timeline(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892b091",
   "metadata": {},
   "source": [
    "## Optional: Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use PyTorch to build a model predicting future product adoption.\n",
    "class CustomerJourneyPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for the prediction model\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col.startswith(('Have_', 'Had_', 'nbr_active_agr_'))]\n",
    "    X = df[feature_cols]\n",
    "    y = df['myTarget']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return torch.FloatTensor(X_scaled), torch.FloatTensor(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and initialize model\n",
    "X, y = prepare_features(combined_df)\n",
    "model = CustomerJourneyPredictor(X.shape[1])\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b702fc",
   "metadata": {},
   "source": [
    "## Additional Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bc443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product combinations analysis\n",
    "def analyze_product_combinations(df):\n",
    "    \"\"\"Analyze which products are commonly held together\"\"\"\n",
    "    have_cols = [col for col in df.columns if col.startswith('Have_')]\n",
    "    product_combinations = df[have_cols].sum()\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df[have_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Product Combination Correlations')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return product_combinations\n",
    "\n",
    "print(\"Product ownership analysis:\")\n",
    "display(analyze_product_combinations(combined_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
