{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9e8279",
   "metadata": {},
   "source": [
    "# Customer Journey Analysis\n",
    "### This notebook analyzes customer journeys across different products, visualizing patterns in purchasing behavior, demographics, and product adoption sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e994e0",
   "metadata": {},
   "source": [
    "## Import and plot-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use(\"seaborn-v0_8-dark-palette\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcd216",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load all ABT_score files and combine them with appropriate target labels.\n",
    "def load_abt_files():\n",
    "    \"\"\"Load all ABT_score files and combine them with appropriate target labels\"\"\"\n",
    "    abt_files = list(Path('../data').glob('ABT_[Ss]core_*.csv'))\n",
    "    \n",
    "    if not abt_files:\n",
    "        print(\"No ABT_score_*.csv files found in current directory!\")\n",
    "        print(\"\\nCurrent directory contents:\")\n",
    "        print([f.name for f in Path('../data').glob('*')])\n",
    "        print(\"\\nPlease ensure your ABT_score_*.csv files are in the data directory.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(abt_files)} ABT_score files:\")\n",
    "    for f in abt_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in abt_files:\n",
    "        product = file_path.stem.split('_')[-1]\n",
    "        try:\n",
    "            print(f\"\\nLoading {product} data...\")\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"Successfully loaded {len(df)} rows for {product}\")\n",
    "            df['product_type'] = product\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Load the data\n",
    "combined_df = load_abt_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a55e99",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and preprocess the combined dataset\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [col for col in df.columns if 'Date' in col or 'date' in col or \n",
    "                   col.startswith(('mFirst_', 'mLast_'))]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Fill numeric NaNs with 0\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    \n",
    "    # Convert binary columns to int\n",
    "    binary_columns = [col for col in df.columns if col.startswith(('Have_', 'Had_', 'Optout_'))]\n",
    "    for col in binary_columns:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "combined_df = preprocess_data(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cec7a",
   "metadata": {},
   "source": [
    "## Customer Journey Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the sequence of products purchased by customers.\n",
    "def analyze_product_sequence(df):\n",
    "    \"\"\"Analyze the sequence of products purchased by customers\"\"\"\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"Empty dataframe provided!\")\n",
    "        \n",
    "    product_cols = [col for col in df.columns if col.startswith('mFirst_')]\n",
    "    if not product_cols:\n",
    "        raise ValueError(\"No product columns found!\")\n",
    "    \n",
    "    timeline_data = []\n",
    "    customer_journeys = {}\n",
    "    \n",
    "    for customer_id in df['sCustomerNaturalKey'].unique():\n",
    "        customer_data = df[df['sCustomerNaturalKey'] == customer_id]\n",
    "        \n",
    "        # Get product acquisition dates\n",
    "        products = []\n",
    "        for col in product_cols:\n",
    "            product = col.replace('mFirst_', '')\n",
    "            date = customer_data[col].iloc[0]\n",
    "            if pd.notna(date):\n",
    "                products.append({\n",
    "                    'sCustomerNaturalKey': customer_id,\n",
    "                    'product': product,\n",
    "                    'acquisition_date': date\n",
    "                })\n",
    "        \n",
    "        # Sort products by date\n",
    "        products = sorted(products, key=lambda x: x['acquisition_date'])\n",
    "        timeline_data.extend(products)\n",
    "        \n",
    "        # Create journey sequence\n",
    "        if products:\n",
    "            journey = ' → '.join([p['product'] for p in products])\n",
    "            customer_journeys[customer_id] = {\n",
    "                'sequence': journey,\n",
    "                'length': len(products),\n",
    "                'duration_days': (products[-1]['acquisition_date'] - products[0]['acquisition_date']).days,\n",
    "                'first_product': products[0]['product'],\n",
    "                'last_product': products[-1]['product']\n",
    "            }\n",
    "    \n",
    "    return pd.DataFrame(timeline_data), pd.DataFrame.from_dict(customer_journeys, orient='index')\n",
    "\n",
    "# Analyze product sequences\n",
    "timeline_df, journey_df = analyze_product_sequence(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e9309",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_customer_journey_sankey(journey_df, max_paths=20, min_customers=50):\n",
    "    \"\"\"\n",
    "    Create an improved Sankey diagram showing actual journey flows\n",
    "    \n",
    "    Parameters:\n",
    "    journey_df: DataFrame with customer journeys\n",
    "    max_paths: Maximum number of unique paths to show\n",
    "    min_customers: Minimum number of customers for a path to be included\n",
    "    \"\"\"\n",
    "    # Get sequences with their counts\n",
    "    sequence_counts = journey_df['sequence'].value_counts()\n",
    "    sequence_counts = sequence_counts[sequence_counts >= min_customers].head(max_paths)\n",
    "    \n",
    "    if len(sequence_counts) == 0:\n",
    "        print(f\"No paths found with at least {min_customers} customers. Reducing minimum threshold...\")\n",
    "        sequence_counts = journey_df['sequence'].value_counts().head(max_paths)\n",
    "        \n",
    "    if len(sequence_counts) == 0:\n",
    "        raise ValueError(\"No valid paths found in the data!\")\n",
    "    \n",
    "    print(f\"Plotting {len(sequence_counts)} unique customer journey paths\")\n",
    "    print(\"\\nPaths being plotted:\")\n",
    "    for path, count in sequence_counts.items():\n",
    "        print(f\"{path}: {count} customers\")\n",
    "    \n",
    "    # Create nodes and links\n",
    "    nodes = set()\n",
    "    links = []\n",
    "    link_values = []\n",
    "    \n",
    "    # Add \"Start\" node\n",
    "    nodes.add(\"Start\")\n",
    "    \n",
    "    for sequence, count in sequence_counts.items():\n",
    "        products = sequence.split(' → ')\n",
    "        \n",
    "        # Add all products to nodes\n",
    "        nodes.update(products)\n",
    "        \n",
    "        # Add link from Start to first product\n",
    "        links.append((\"Start\", products[0]))\n",
    "        link_values.append(count)\n",
    "        \n",
    "        # Create links between consecutive products\n",
    "        for i in range(len(products) - 1):\n",
    "            links.append((products[i], products[i + 1]))\n",
    "            link_values.append(count)\n",
    "    \n",
    "    # Convert nodes to list and create node indices\n",
    "    nodes = list(nodes)\n",
    "    node_indices = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    # Create color scale - fixed to avoid division by zero\n",
    "    n_colors = len(nodes)\n",
    "    if n_colors > 0:\n",
    "        # Use a fixed set of colors instead of trying to slice\n",
    "        node_colors = px.colors.sequential.Blues[1:n_colors+1] if n_colors <= len(px.colors.sequential.Blues) else \\\n",
    "                     [px.colors.sequential.Blues[i % len(px.colors.sequential.Blues)] for i in range(n_colors)]\n",
    "    else:\n",
    "        node_colors = [px.colors.sequential.Blues[1]]  # Default color if something goes wrong\n",
    "    \n",
    "    # Create Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=20,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=nodes,\n",
    "            color=node_colors,\n",
    "            hovertemplate='Node: %{label}<br>Total Flow: %{value}<extra></extra>'\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=[node_indices[link[0]] for link in links],\n",
    "            target=[node_indices[link[1]] for link in links],\n",
    "            value=link_values,\n",
    "            hovertemplate='From: %{source.label}<br>To: %{target.label}<br>Flow: %{value}<extra></extra>'\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"Customer Journey Paths Analysis\",\n",
    "            x=0.5,\n",
    "            y=0.95,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        font_size=12,\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_enhanced_journey_insights(timeline_df, journey_df, combined_df):\n",
    "    \"\"\"Create comprehensive journey visualizations\"\"\"\n",
    "    if len(timeline_df) == 0 or len(journey_df) == 0:\n",
    "        print(\"Warning: Empty data provided for visualization\")\n",
    "        return\n",
    "    \n",
    "    # 1. Journey Length Distribution with statistics\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create subplot grid\n",
    "    gs = plt.GridSpec(1, 2, width_ratios=[2, 1])\n",
    "    \n",
    "    # Journey length histogram\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    sns.histplot(data=journey_df, x='length', bins=20, ax=ax1)\n",
    "    ax1.set_title('Distribution of Journey Lengths')\n",
    "    ax1.set_xlabel('Number of Products')\n",
    "    ax1.set_ylabel('Number of Customers')\n",
    "    \n",
    "    # Add statistics table\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "    stats = journey_df['length'].describe()\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    table_data = [[f\"{k}: {v:.2f}\"] for k, v in stats.items()]\n",
    "    ax2.table(cellText=table_data, \n",
    "              colLabels=['Journey Length Statistics'],\n",
    "              cellLoc='left',\n",
    "              loc='center',\n",
    "              bbox=[0.1, 0.2, 0.8, 0.6])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Enhanced Product Adoption Timeline\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Calculate point sizes beforehand\n",
    "    product_counts = timeline_df.groupby('product').size()\n",
    "    size_map = product_counts.map(lambda x: 100 + (x/product_counts.max() * 300))\n",
    "    \n",
    "    # Create main scatter plot\n",
    "    sns.scatterplot(data=timeline_df, \n",
    "                   x='acquisition_date', \n",
    "                   y='product', \n",
    "                   alpha=0.6,\n",
    "                   hue='product',\n",
    "                   size='product',\n",
    "                   sizes=(100, 400))\n",
    "    \n",
    "    # Add trend lines\n",
    "    for product in timeline_df['product'].unique():\n",
    "        product_data = timeline_df[timeline_df['product'] == product]\n",
    "        if len(product_data) > 1:  # Need at least 2 points for a trend line\n",
    "            z = np.polyfit(product_data['acquisition_date'].astype(np.int64), \n",
    "                          range(len(product_data)), 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(product_data['acquisition_date'], \n",
    "                    p(product_data['acquisition_date'].astype(np.int64)), \n",
    "                    '--', alpha=0.5)\n",
    "    \n",
    "    plt.title('Product Adoption Timeline with Trends')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "    # 3. Enhanced Product Correlation Heatmap\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Get product correlations\n",
    "    have_cols = [col for col in combined_df.columns if col.startswith('Have_')]\n",
    "    corr_matrix = combined_df[have_cols].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix))\n",
    "    \n",
    "    # Create heatmap with enhanced styling\n",
    "    sns.heatmap(corr_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdYlBu',\n",
    "                center=0,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "    # Clean up labels\n",
    "    labels = [col.replace('Have_', '') for col in have_cols]\n",
    "    plt.xticks(np.arange(len(labels)) + 0.5, labels, rotation=45, ha='right')\n",
    "    plt.yticks(np.arange(len(labels)) + 0.5, labels, rotation=0)\n",
    "    \n",
    "    plt.title('Product Adoption Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Add Journey Flow Analysis\n",
    "    journey_lengths = journey_df['length'].value_counts().sort_index()\n",
    "    product_flows = []\n",
    "    \n",
    "    for length in journey_lengths.index:\n",
    "        journeys = journey_df[journey_df['length'] == length]\n",
    "        flows = journeys['sequence'].value_counts().head(5)\n",
    "        product_flows.append({\n",
    "            'length': length,\n",
    "            'top_flows': flows\n",
    "        })\n",
    "    \n",
    "    print(\"\\nMost Common Journey Flows by Length:\")\n",
    "    for flow in product_flows:\n",
    "        print(f\"\\nJourneys with {flow['length']} products ({journey_lengths[flow['length']]} customers):\")\n",
    "        for path, count in flow['top_flows'].items():\n",
    "            print(f\"  {path}: {count} customers\")\n",
    "\n",
    "# Create visualizations\n",
    "sankey_fig = plot_customer_journey_sankey(journey_df, max_paths=30, min_customers=5)\n",
    "sankey_fig.show()\n",
    "create_enhanced_journey_insights(timeline_df, journey_df, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34349a99-5880-446d-a287-6f6be2eb235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_starter_products(journey_df, combined_df):\n",
    "    \"\"\"Analyze the most common starter products and their subsequent journeys\"\"\"\n",
    "    \n",
    "    # Get top 3 starter products\n",
    "    top_starters = journey_df['first_product'].value_counts().head(3)\n",
    "    \n",
    "    # For each top starter, analyze the typical journey\n",
    "    starter_insights = {}\n",
    "    for product in top_starters.index:\n",
    "        # Get customers who started with this product\n",
    "        starter_journeys = journey_df[journey_df['first_product'] == product]\n",
    "        \n",
    "        # Get customer IDs for demographic analysis\n",
    "        customer_ids = starter_journeys.index\n",
    "        customer_data = combined_df[combined_df['sCustomerNaturalKey'].isin(customer_ids)]\n",
    "        \n",
    "        insights = {\n",
    "            'total_customers': len(starter_journeys),\n",
    "            'avg_journey_length': starter_journeys['length'].mean(),\n",
    "            'avg_journey_duration': starter_journeys['duration_days'].mean(),\n",
    "            'common_next_products': starter_journeys[starter_journeys['length'] > 1]['sequence'].apply(\n",
    "                lambda x: x.split(' → ')[1] if ' → ' in x else None\n",
    "            ).value_counts().head(3),\n",
    "            'customer_profile': {\n",
    "                'avg_age': customer_data['Age'].mean(),\n",
    "                'pct_women': (customer_data['Woman'] == 1).mean() * 100,\n",
    "                'pct_apartment': (customer_data['Apartment'] == 1).mean() * 100,\n",
    "                'common_lifestyle': customer_data['LifestyleGroupCode'].mode().iloc[0]\n",
    "            }\n",
    "        }\n",
    "        starter_insights[product] = insights\n",
    "    \n",
    "    return starter_insights, top_starters\n",
    "\n",
    "def visualize_starter_product_journeys(journey_df, top_starters):\n",
    "    \"\"\"Create visualizations for top starter product journeys\"\"\"\n",
    "    \n",
    "    # 1. Sankey diagram for each starter product\n",
    "    for product in top_starters.index:\n",
    "        starter_journeys = journey_df[journey_df['first_product'] == product]\n",
    "        \n",
    "        # Create Sankey for this starter product\n",
    "        fig = plot_customer_journey_sankey(starter_journeys, max_paths=5)\n",
    "        fig.update_layout(title=f\"Customer Journeys Starting with {product}\")\n",
    "        fig.show()\n",
    "    \n",
    "    # 2. Journey length comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    journey_lengths = []\n",
    "    for product in top_starters.index:\n",
    "        lengths = journey_df[journey_df['first_product'] == product]['length']\n",
    "        journey_lengths.append(lengths)\n",
    "    \n",
    "    plt.boxplot(journey_lengths, labels=top_starters.index)\n",
    "    plt.title('Journey Lengths by Starter Product')\n",
    "    plt.ylabel('Number of Products')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Time to second product\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    time_to_second = []\n",
    "    labels = []\n",
    "    for product in top_starters.index:\n",
    "        starter_journeys = journey_df[journey_df['first_product'] == product]\n",
    "        multi_product = starter_journeys[starter_journeys['length'] > 1]\n",
    "        if len(multi_product) > 0:\n",
    "            time_to_second.append(multi_product['duration_days'] / multi_product['length'])\n",
    "            labels.append(product)\n",
    "    \n",
    "    plt.boxplot(time_to_second, labels=labels)\n",
    "    plt.title('Time to Second Product by Starter Product')\n",
    "    plt.ylabel('Days')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "starter_insights, top_starters = analyze_starter_products(journey_df, combined_df)\n",
    "print(\"\\nTop Starter Product Insights:\")\n",
    "for product, insights in starter_insights.items():\n",
    "    print(f\"\\n{product}:\")\n",
    "    print(f\"Total Customers: {insights['total_customers']:,}\")\n",
    "    print(f\"Average Journey Length: {insights['avg_journey_length']:.2f} products\")\n",
    "    print(f\"Average Journey Duration: {insights['avg_journey_duration']:.1f} days\")\n",
    "    print(\"\\nCommon Next Products:\")\n",
    "    print(insights['common_next_products'])\n",
    "    print(\"\\nCustomer Profile:\")\n",
    "    for key, value in insights['customer_profile'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create visualizations\n",
    "visualize_starter_product_journeys(journey_df, top_starters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c145a0-bf6b-4cf8-a875-aee44cd3852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_customer_segments(journey_df, combined_df):\n",
    "    \"\"\"Analyze customer segments based on their journeys\"\"\"\n",
    "    \n",
    "    # Create customer segments\n",
    "    segments = pd.DataFrame()\n",
    "    segments['journey_length'] = journey_df['length']\n",
    "    segments['journey_duration'] = journey_df['duration_days']\n",
    "    segments['first_product'] = journey_df['first_product']\n",
    "    \n",
    "    # Add customer demographics - handling duplicates by taking first occurrence\n",
    "    demographics = combined_df.drop_duplicates(subset='sCustomerNaturalKey', keep='first').set_index('sCustomerNaturalKey')\n",
    "    segments['age'] = demographics.loc[journey_df.index, 'Age']\n",
    "    segments['is_woman'] = demographics.loc[journey_df.index, 'Woman']\n",
    "    segments['lifestyle'] = demographics.loc[journey_df.index, 'LifestyleGroupCode']\n",
    "    \n",
    "    # Create segment labels\n",
    "    segments['segment'] = pd.qcut(segments['journey_length'], q=3, labels=['Basic', 'Moderate', 'Extensive'])\n",
    "    \n",
    "    # Analyze segments\n",
    "    segment_insights = {}\n",
    "    for segment in segments['segment'].unique():\n",
    "        segment_data = segments[segments['segment'] == segment]\n",
    "        insights = {\n",
    "            'size': len(segment_data),\n",
    "            'avg_products': segment_data['journey_length'].mean(),\n",
    "            'avg_duration': segment_data['journey_duration'].mean(),\n",
    "            'common_starter': segment_data['first_product'].mode().iloc[0],\n",
    "            'avg_age': segment_data['age'].mean(),\n",
    "            'pct_women': (segment_data['is_woman'] == 1).mean() * 100,\n",
    "            'common_lifestyle': segment_data['lifestyle'].mode().iloc[0]\n",
    "        }\n",
    "        segment_insights[segment] = insights\n",
    "    \n",
    "    return segment_insights, segments\n",
    "\n",
    "# Analyze customer segments\n",
    "segment_insights, segments = analyze_customer_segments(journey_df, combined_df)\n",
    "print(\"\\nCustomer Segment Insights:\")\n",
    "for segment, insights in segment_insights.items():\n",
    "    print(f\"\\n{segment} Segment:\")\n",
    "    for key, value in insights.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b702fc",
   "metadata": {},
   "source": [
    "## Additional Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3b0d7-84b3-4010-b2f5-735186e2aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journey Pattern Analysis\n",
    "def analyze_journey_patterns(journey_df, combined_df):\n",
    "    \"\"\"Analyze patterns in customer journeys\"\"\"\n",
    "    patterns = {\n",
    "        'journey_stats': {\n",
    "            'total_customers': len(journey_df),\n",
    "            'avg_products': journey_df['length'].mean(),\n",
    "            'avg_duration': journey_df['duration_days'].mean(),\n",
    "            'common_first': journey_df['first_product'].value_counts().head(),\n",
    "            'common_last': journey_df['last_product'].value_counts().head()\n",
    "        },\n",
    "        'journey_segments': {\n",
    "            'single_product': (journey_df['length'] == 1).mean(),\n",
    "            'short_journey': ((journey_df['length'] > 1) & (journey_df['length'] <= 3)).mean(),\n",
    "            'long_journey': (journey_df['length'] > 3).mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Analyze patterns\n",
    "patterns = analyze_journey_patterns(journey_df, combined_df)\n",
    "print(\"\\nJourney Analysis Results:\")\n",
    "for category, stats in patterns.items():\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bc443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product combinations analysis\n",
    "def analyze_product_combinations(df):\n",
    "    \"\"\"Analyze which products are commonly held together\"\"\"\n",
    "    have_cols = [col for col in df.columns if col.startswith('Have_')]\n",
    "    product_combinations = df[have_cols].sum()\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df[have_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Product Combination Correlations')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return product_combinations\n",
    "\n",
    "print(\"Product ownership analysis:\")\n",
    "display(analyze_product_combinations(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2c539-0042-40ba-9a00-27294d06fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_customer_segments(segments):\n",
    "    \"\"\"Create visualizations for customer segments\"\"\"\n",
    "    \n",
    "    # 1. Segment size comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    segments['segment'].value_counts().plot(kind='bar')\n",
    "    plt.title('Size of Customer Segments')\n",
    "    plt.xlabel('Segment')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Age distribution by segment\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=segments, x='segment', y='age')\n",
    "    plt.title('Age Distribution by Customer Segment')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Product mix by segment\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    segments.groupby('segment')['first_product'].value_counts(normalize=True).unstack().plot(kind='bar', stacked=True)\n",
    "    plt.title('First Product Distribution by Segment')\n",
    "    plt.xlabel('Segment')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.legend(title='First Product', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize insights\n",
    "visualize_customer_segments(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceffdc7-b888-411c-904f-9ea82a5b61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retention_patterns(journey_df, combined_df):\n",
    "    \"\"\"Analyze customer retention patterns\"\"\"\n",
    "    retention_data = pd.DataFrame()\n",
    "    \n",
    "    # Time between purchases\n",
    "    retention_data['avg_purchase_gap'] = journey_df['duration_days'] / (journey_df['length'] - 1)\n",
    "    \n",
    "    # Product stickiness\n",
    "    have_cols = [col for col in combined_df.columns if col.startswith('Have_')]\n",
    "    had_cols = [col for col in combined_df.columns if col.startswith('Had_')]\n",
    "    \n",
    "    for have, had in zip(have_cols, had_cols):\n",
    "        product = have.replace('Have_', '')\n",
    "        retention_data[f'{product}_retention'] = combined_df[have] / combined_df[had]\n",
    "    \n",
    "    return retention_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51488e15-ed60-49e0-b309-0ade6bcbf72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_engagement_score(journey_df, combined_df):\n",
    "    \"\"\"Calculate customer engagement scores based on multiple factors\"\"\"\n",
    "    engagement = pd.DataFrame()\n",
    "    \n",
    "    # Product diversity score\n",
    "    engagement['product_diversity'] = journey_df['length'] / len([col for col in combined_df.columns if col.startswith('Have_')])\n",
    "    \n",
    "    # Engagement speed\n",
    "    engagement['engagement_speed'] = journey_df['length'] / journey_df['duration_days'].clip(1)\n",
    "    \n",
    "    # Activity score\n",
    "    engagement['activity_score'] = combined_df[[col for col in combined_df.columns if col.startswith('nbr_active_agr_')]].sum(axis=1)\n",
    "    \n",
    "    # Overall score\n",
    "    engagement['total_score'] = (engagement['product_diversity'] + \n",
    "                               engagement['engagement_speed'] + \n",
    "                               engagement['activity_score']).rank(pct=True)\n",
    "    \n",
    "    return engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabb99f-9b31-490e-9040-9748582175b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(journey_df, timeline_df):\n",
    "    \"\"\"Analyze how customer journeys evolve over time\"\"\"\n",
    "    # Seasonal patterns\n",
    "    timeline_df['month'] = timeline_df['acquisition_date'].dt.month\n",
    "    timeline_df['year'] = timeline_df['acquisition_date'].dt.year\n",
    "    \n",
    "    # Time between products\n",
    "    journey_df['avg_time_between_products'] = journey_df['duration_days'] / (journey_df['length'] - 1)\n",
    "    journey_df['avg_time_between_products'] = journey_df['avg_time_between_products'].fillna(0)\n",
    "    \n",
    "    # Product velocity\n",
    "    yearly_patterns = timeline_df.groupby(['year', 'product']).size().unstack(fill_value=0)\n",
    "    \n",
    "    return {\n",
    "        'seasonal_patterns': timeline_df.groupby('month')['product'].value_counts(),\n",
    "        'yearly_trends': yearly_patterns,\n",
    "        'avg_acquisition_speed': journey_df['avg_time_between_products'].describe()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d012e-88d7-4e42-866a-9cc6699b77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_customer_value(journey_df, combined_df):\n",
    "    \"\"\"Analyze relationship between journey patterns and customer value\"\"\"\n",
    "    value_analysis = pd.DataFrame()\n",
    "    value_analysis['journey_length'] = journey_df['length']\n",
    "    value_analysis['total_products'] = journey_df['length']\n",
    "    value_analysis['journey_duration'] = journey_df['duration_days']\n",
    "    \n",
    "    # Add product counts\n",
    "    have_cols = [col for col in combined_df.columns if col.startswith('Have_')]\n",
    "    value_analysis['total_active_products'] = combined_df[have_cols].sum(axis=1)\n",
    "    \n",
    "    # Analyze cross-sell success\n",
    "    value_analysis['cross_sell_ratio'] = value_analysis['total_active_products'] / value_analysis['journey_length']\n",
    "    \n",
    "    return value_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece54393-f663-4544-ad29-6b03bae4f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_customer_lifetime_value(journey_df, combined_df):\n",
    "    \"\"\"Analyze customer lifetime value based on product portfolio\"\"\"\n",
    "    value_analysis = pd.DataFrame()\n",
    "    \n",
    "    # Portfolio size value\n",
    "    value_analysis['portfolio_value'] = journey_df['length']\n",
    "    \n",
    "    # Product mix value\n",
    "    have_cols = [col for col in combined_df.columns if col.startswith('Have_')]\n",
    "    value_analysis['product_mix'] = combined_df[have_cols].sum(axis=1)\n",
    "    \n",
    "    # Engagement duration value\n",
    "    value_analysis['engagement_duration'] = journey_df['duration_days']\n",
    "    \n",
    "    return value_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9522294-f38e-42ff-96cd-ddd0e065b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_product_affinities(combined_df):\n",
    "    \"\"\"Analyze detailed product relationships and purchase patterns\"\"\"\n",
    "    have_cols = [col for col in combined_df.columns if col.startswith('Have_')]\n",
    "    \n",
    "    # Product co-occurrence\n",
    "    product_pairs = []\n",
    "    for i, prod1 in enumerate(have_cols):\n",
    "        for prod2 in have_cols[i+1:]:\n",
    "            together = ((combined_df[prod1] == 1) & (combined_df[prod2] == 1)).sum()\n",
    "            total_prod1 = (combined_df[prod1] == 1).sum()\n",
    "            total_prod2 = (combined_df[prod2] == 1).sum()\n",
    "            \n",
    "            if total_prod1 > 0 and total_prod2 > 0:\n",
    "                lift = (together / len(combined_df)) / ((total_prod1 / len(combined_df)) * (total_prod2 / len(combined_df)))\n",
    "                product_pairs.append({\n",
    "                    'product1': prod1.replace('Have_', ''),\n",
    "                    'product2': prod2.replace('Have_', ''),\n",
    "                    'together_count': together,\n",
    "                    'lift': lift\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(product_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca56592-78a1-4d96-b855-15fee547fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lifecycle_stages(journey_df, combined_df):\n",
    "    \"\"\"Analyze customer lifecycle stages and transitions\"\"\"\n",
    "    lifecycle_data = pd.DataFrame()\n",
    "    \n",
    "    # Define lifecycle stages based on journey length\n",
    "    lifecycle_data['stage'] = pd.cut(\n",
    "        journey_df['length'], \n",
    "        bins=[0, 1, 3, 5, float('inf')],\n",
    "        labels=['New', 'Growing', 'Established', 'Mature'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Calculate adoption rate\n",
    "    lifecycle_data['adoption_rate'] = journey_df['length'] / journey_df['duration_days']\n",
    "    lifecycle_data['adoption_rate'] = lifecycle_data['adoption_rate'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Drop any rows where either stage or adoption_rate is null\n",
    "    lifecycle_data = lifecycle_data.dropna(subset=['stage', 'adoption_rate'])\n",
    "    \n",
    "    # Ensure adoption_rate is not infinite\n",
    "    lifecycle_data = lifecycle_data[lifecycle_data['adoption_rate'] < 1]  # Filter out unrealistic rates\n",
    "    \n",
    "    return lifecycle_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6ef94-07f1-49d8-9530-71090cbb64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_churn_risk(journey_df, combined_df):\n",
    "    \"\"\"Analyze potential churn indicators in customer journeys\"\"\"\n",
    "    risk_factors = pd.DataFrame()\n",
    "    \n",
    "    # Time since last product\n",
    "    current_date = combined_df['mFirst_BankBolan'].max()  # Use as reference date\n",
    "    # Using the last product's acquisition date\n",
    "    for idx in journey_df.index:\n",
    "        last_product = journey_df.loc[idx, 'last_product']\n",
    "        last_date = timeline_df[timeline_df['product'] == last_product]['acquisition_date'].max()\n",
    "        risk_factors.loc[idx, 'days_since_last_product'] = (current_date - last_date).days\n",
    "    \n",
    "    # Product discontinuation\n",
    "    had_cols = [col for col in combined_df.columns if col.startswith('Had_')]\n",
    "    have_cols = [col.replace('Had_', 'Have_') for col in had_cols]\n",
    "    \n",
    "    risk_factors['discontinued_products'] = 0\n",
    "    for had, have in zip(had_cols, have_cols):\n",
    "        risk_factors['discontinued_products'] += (combined_df[had] > combined_df[have]).astype(int)\n",
    "    \n",
    "    return risk_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc03b8-ab1c-4148-a6b8-0cd45935fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_journey_visualization(journey_df, timeline_df, combined_df):\n",
    "    \"\"\"Create comprehensive journey visualizations including all aspects\"\"\"\n",
    "    # Create figure with proper grid\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    gs = fig.add_gridspec(4, 2, height_ratios=[1, 1, 1, 1])\n",
    "    \n",
    "    # 1. Temporal Patterns\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    temporal_patterns = analyze_temporal_patterns(journey_df, timeline_df)\n",
    "    if temporal_patterns['yearly_trends'].size > 0:  # Check if we have data\n",
    "        sns.heatmap(temporal_patterns['yearly_trends'], cmap='YlOrRd', ax=ax1)\n",
    "        ax1.set_title('Temporal Product Adoption Patterns')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No temporal data available', ha='center')\n",
    "    \n",
    "    # 2. Customer Value\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    value_analysis = analyze_customer_value(journey_df, combined_df)\n",
    "    if not value_analysis.empty:\n",
    "        sns.scatterplot(data=value_analysis, x='journey_length', y='total_active_products', ax=ax2)\n",
    "        ax2.set_title('Journey Length vs Active Products')\n",
    "    \n",
    "    # 3. Product Affinities\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    affinities = analyze_product_affinities(combined_df)\n",
    "    if not affinities.empty:\n",
    "        affinity_pivot = affinities.pivot('product1', 'product2', 'lift')\n",
    "        sns.heatmap(affinity_pivot, annot=True, ax=ax3, cmap='YlOrRd')\n",
    "        ax3.set_title('Product Affinity Analysis')\n",
    "    \n",
    "    # 4. Lifecycle Stages\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    lifecycle_data = analyze_lifecycle_stages(journey_df, combined_df)\n",
    "    if not lifecycle_data.empty:\n",
    "        # Create violin plot instead of boxplot for better visualization\n",
    "        sns.violinplot(data=lifecycle_data, x='stage', y='adoption_rate', ax=ax4)\n",
    "        ax4.set_title('Adoption Rate by Lifecycle Stage')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Churn Risk\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    risk_data = analyze_churn_risk(journey_df, combined_df)\n",
    "    if not risk_data.empty:\n",
    "        sns.histplot(data=risk_data, x='days_since_last_product', ax=ax5)\n",
    "        ax5.set_title('Days Since Last Product (Churn Risk)')\n",
    "    \n",
    "    # 6. Additional Insights\n",
    "    ax6 = fig.add_subplot(gs[3, :])\n",
    "    journey_summary = pd.DataFrame({\n",
    "        'Metric': ['Total Customers', 'Average Journey Length', 'Average Journey Duration (days)'],\n",
    "        'Value': [\n",
    "            f\"{len(journey_df):,}\",\n",
    "            f\"{journey_df['length'].mean():.2f}\",\n",
    "            f\"{journey_df['duration_days'].mean():.1f}\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Create table with no cells\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(\n",
    "        cellText=journey_summary.values,\n",
    "        colLabels=journey_summary.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        bbox=[0.2, 0.2, 0.6, 0.6]\n",
    "    )\n",
    "    \n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    for k, cell in table._cells.items():\n",
    "        cell.set_edgecolor('white')\n",
    "        if k[0] == 0:  # Header\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    ax6.set_title('Journey Summary Statistics', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create and display visualization\n",
    "fig = create_comprehensive_journey_visualization(journey_df, timeline_df, combined_df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892b091",
   "metadata": {},
   "source": [
    "## Optional: Predictive Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c841c07-a9ab-463e-b86b-e4538351d04b",
   "metadata": {},
   "source": [
    "class CustomerJourneyPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for the prediction model\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col.startswith(('Have_', 'Had_', 'nbr_active_agr_'))]\n",
    "    X = df[feature_cols]\n",
    "    y = df['myTarget']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return torch.FloatTensor(X_scaled), torch.FloatTensor(y.values)\n",
    "\n",
    "# Prepare data and initialize model\n",
    "X, y = prepare_features(combined_df)\n",
    "model = CustomerJourneyPredictor(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e454d-80a4-4b96-beed-f86bfedf0123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
