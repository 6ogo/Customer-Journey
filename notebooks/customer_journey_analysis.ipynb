{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9e8279",
   "metadata": {},
   "source": [
    "# Customer Journey Analysis\n",
    "# This notebook analyzes customer journeys across different products, visualizing patterns in purchasing behavior, demographics, and product adoption sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e994e0",
   "metadata": {},
   "source": [
    "## Import and plot-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use(\"seaborn-v0_8-dark-palette\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcd216",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load all ABT_score files and combine them with appropriate target labels.\n",
    "def load_abt_files():\n",
    "    \"\"\"Load all ABT_score files and combine them with appropriate target labels\"\"\"\n",
    "    # List all ABT_Score/ABT_score files in current directory (case insensitive)\n",
    "    abt_files = list(Path('../data').glob('ABT_[Ss]core_*.csv'))\n",
    "    \n",
    "    # Check if any files were found\n",
    "    if not abt_files:\n",
    "        print(\"No ABT_score_*.csv files found in current directory!\")\n",
    "        print(\"\\nCurrent directory contents:\")\n",
    "        print([f.name for f in Path('../data').glob('*')])\n",
    "        print(\"\\nPlease ensure your ABT_score_*.csv files are in the data directory.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(abt_files)} ABT_score files:\")\n",
    "    for f in abt_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in abt_files:\n",
    "        product = file_path.stem.split('_')[-1]\n",
    "        try:\n",
    "            print(f\"\\nLoading {product} data...\")\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"Successfully loaded {len(df)} rows for {product}\")\n",
    "            df['product_type'] = product\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data frames were successfully loaded. Please check the file format and contents.\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Load the data\n",
    "combined_df = load_abt_files()\n",
    "\n",
    "if combined_df is not None:\n",
    "    # Display basic information about the dataset\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"Total number of records: {len(combined_df)}\")\n",
    "    print(\"\\nProduct distribution:\")\n",
    "    print(combined_df['product_type'].value_counts())\n",
    "else:\n",
    "    print(\"\\nPlease fix the data loading issues before continuing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a55e99",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and preprocess the combined dataset\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Store original NaN locations for each column\n",
    "    nan_masks = {}\n",
    "    for col in df.columns:\n",
    "        nan_masks[col] = df[col].isna()\n",
    "    \n",
    "    # Print NaN statistics before conversion\n",
    "    print(\"\\nNaN Statistics before conversion:\")\n",
    "    nan_stats = df.isna().sum()\n",
    "    print(nan_stats[nan_stats > 0])\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [col for col in df.columns if 'Date' in col or 'date' in col]\n",
    "    for col in date_columns:\n",
    "        # Keep track of which dates were NaN\n",
    "        df[f\"{col}_was_nan\"] = df[col].isna()\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert numeric columns with NaN to 0\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        # Keep track of which values were NaN\n",
    "        df[f\"{col}_was_nan\"] = df[col].isna()\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Convert binary columns to int\n",
    "    binary_columns = [col for col in df.columns if col.startswith(('Have_', 'Had_', 'Optout_'))]\n",
    "    for col in binary_columns:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "    \n",
    "    # Store nan_masks in the dataframe metadata\n",
    "    df.attrs['nan_masks'] = nan_masks\n",
    "    \n",
    "    print(\"\\nColumns with their NaN conversion flags added:\")\n",
    "    nan_flag_cols = [col for col in df.columns if col.endswith('_was_nan')]\n",
    "    print(f\"Added {len(nan_flag_cols)} NaN flag columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_valid_data(df, column, include_converted_nans=False):\n",
    "    \"\"\"\n",
    "    Get data excluding NaN values for specific column\n",
    "    \n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "    column : str - column name to analyze\n",
    "    include_converted_nans : bool - if True, includes values that were converted from NaN to 0\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with only valid data for the specified column\n",
    "    \"\"\"\n",
    "    if f\"{column}_was_nan\" in df.columns and not include_converted_nans:\n",
    "        return df[~df[f\"{column}_was_nan\"]]\n",
    "    return df\n",
    "\n",
    "def analyze_column_with_nans(df, column):\n",
    "    \"\"\"\n",
    "    Analyze a column's statistics both including and excluding converted NaN values\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalysis for column: {column}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    # All data including converted NaNs\n",
    "    print(\"Including converted NaN values:\")\n",
    "    print(df[column].describe())\n",
    "    \n",
    "    # Only original non-NaN values\n",
    "    if f\"{column}_was_nan\" in df.columns:\n",
    "        valid_data = get_valid_data(df, column)\n",
    "        print(\"\\nExcluding converted NaN values:\")\n",
    "        print(valid_data[column].describe())\n",
    "        \n",
    "        print(f\"\\nTotal values: {len(df)}\")\n",
    "        print(f\"Original NaN values: {df[f'{column}_was_nan'].sum()}\")\n",
    "        print(f\"Valid values: {len(valid_data)}\")\n",
    "    else:\n",
    "        print(\"\\nNo NaN tracking available for this column\")\n",
    "\n",
    "combined_df = preprocess_data(combined_df)\n",
    "\n",
    "# Display sample of preprocessed data\n",
    "print(\"Sample of preprocessed data:\")\n",
    "display(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cec7a",
   "metadata": {},
   "source": [
    "## Customer Journey Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the sequence of products purchased by customers.\n",
    "def analyze_product_sequence(df):\n",
    "    \"\"\"Analyze the sequence of products purchased by customers\"\"\"\n",
    "    # Filter out _was_nan columns and get product columns\n",
    "    product_cols = [col for col in df.columns \n",
    "                   if col.startswith('mFirst_') and not col.endswith('_was_nan')]\n",
    "    \n",
    "    # Create a timeline of product acquisitions\n",
    "    timeline_data = []\n",
    "    \n",
    "    for customer_id in df['sCustomerNaturalKey'].unique():\n",
    "        customer_data = df[df['sCustomerNaturalKey'] == customer_id]\n",
    "        \n",
    "        # Get product acquisition dates for this customer\n",
    "        products = []\n",
    "        for col in product_cols:\n",
    "            product = col.replace('mFirst_', '')\n",
    "            date = customer_data[col].iloc[0]\n",
    "            if pd.notna(date):\n",
    "                products.append({\n",
    "                    'sCustomerNaturalKey': customer_id,\n",
    "                    'product': product,\n",
    "                    'acquisition_date': date\n",
    "                })\n",
    "        \n",
    "        # Sort products by date for this customer\n",
    "        products = sorted(products, key=lambda x: x['acquisition_date'])\n",
    "        timeline_data.extend(products)\n",
    "    \n",
    "    return pd.DataFrame(timeline_data)\n",
    "\n",
    "# Analyze product sequences\n",
    "product_timeline = analyze_product_sequence(combined_df)\n",
    "\n",
    "# Display summary of product sequences\n",
    "print(\"Most common first products:\")\n",
    "display(product_timeline.groupby('sCustomerNaturalKey')\n",
    "        .first()['product']\n",
    "        .value_counts()\n",
    "        .head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e9309",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Journey Sankey Diagram\n",
    "def plot_customer_journey_sankey(df):\n",
    "    \"\"\"Create a Sankey diagram of customer journeys\"\"\"\n",
    "    product_sequence = analyze_product_sequence(df)\n",
    "    \n",
    "    # Group by customer and create product sequences\n",
    "    customer_sequences = product_sequence.groupby('sCustomerNaturalKey').agg(\n",
    "        list\n",
    "    )['product'].value_counts().head(10)  # Top 10 most common sequences\n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node = dict(\n",
    "            pad = 15,\n",
    "            thickness = 20,\n",
    "            line = dict(color = \"black\", width = 0.5),\n",
    "            label = customer_sequences.index,\n",
    "            color = \"blue\"\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = [i for i in range(len(customer_sequences)-1)],\n",
    "            target = [i+1 for i in range(len(customer_sequences)-1)],\n",
    "            value = customer_sequences.values[:-1]\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(title_text=\"Most Common Customer Journey Paths\", \n",
    "                     font_size=10,\n",
    "                     height=600)\n",
    "    fig.show()\n",
    "\n",
    "# Create Sankey diagram\n",
    "plot_customer_journey_sankey(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a782b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic Analysis\n",
    "def plot_demographic_distribution(df):\n",
    "    \"\"\"Plot age distribution for different products\"\"\"\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    \n",
    "    # Age distribution\n",
    "    sns.boxplot(x='product_type', y='Age', data=df, ax=ax1)\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "    ax1.set_title('Age Distribution by Product')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional demographic insights\n",
    "    print(\"\\nMean age by product:\")\n",
    "    display(df.groupby('product_type')['Age'].mean().sort_values(ascending=False))\n",
    "\n",
    "plot_demographic_distribution(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Adoption Timeline\n",
    "def plot_product_adoption_timeline(df):\n",
    "    \"\"\"Plot timeline of product adoption\"\"\"\n",
    "    timeline_data = analyze_product_sequence(df)\n",
    "    \n",
    "    fig = px.scatter(timeline_data, \n",
    "                    x='acquisition_date', \n",
    "                    y='product',\n",
    "                    color='product',\n",
    "                    title='Product Adoption Timeline')\n",
    "    \n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    # Additional timeline insights\n",
    "    print(\"\\nMedian time between first and second product (days):\")\n",
    "    customer_products = timeline_data.groupby('sCustomerNaturalKey')\n",
    "    time_between = customer_products.acquisition_date.agg(lambda x: x.diff().median().days)\n",
    "    display(time_between.median())\n",
    "\n",
    "plot_product_adoption_timeline(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892b091",
   "metadata": {},
   "source": [
    "## Optional: Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use PyTorch to build a model predicting future product adoption.\n",
    "class CustomerJourneyPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for the prediction model\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col.startswith(('Have_', 'Had_', 'nbr_active_agr_'))]\n",
    "    X = df[feature_cols]\n",
    "    y = df['myTarget']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return torch.FloatTensor(X_scaled), torch.FloatTensor(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and initialize model\n",
    "X, y = prepare_features(combined_df)\n",
    "model = CustomerJourneyPredictor(X.shape[1])\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b702fc",
   "metadata": {},
   "source": [
    "## Additional Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e311b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NaN statistics and patterns\n",
    "def analyze_nan_patterns(df):\n",
    "    \"\"\"Analyze patterns in NaN values across columns\"\"\"\n",
    "    # Count NaN values per column\n",
    "    nan_counts = df.apply(lambda x: x.endswith('_was_nan') and x.sum())\n",
    "    nan_counts = nan_counts[nan_counts > 0]\n",
    "    \n",
    "    print(\"NaN Statistics by Column:\")\n",
    "    print(\"------------------------\")\n",
    "    for col in nan_counts.index:\n",
    "        original_col = col.replace('_was_nan', '')\n",
    "        print(f\"\\n{original_col}:\")\n",
    "        print(f\"  - Original NaN count: {nan_counts[col]}\")\n",
    "        print(f\"  - Percentage: {(nan_counts[col] / len(df)) * 100:.2f}%\")\n",
    "        \n",
    "        # Show value distribution excluding converted NaNs\n",
    "        valid_data = get_valid_data(df, original_col)\n",
    "        if len(valid_data) > 0:\n",
    "            print(\"  - Statistics for valid values:\")\n",
    "            print(valid_data[original_col].describe())\n",
    "\n",
    "# Analyze NaN patterns\n",
    "analyze_nan_patterns(combined_df)\n",
    "\n",
    "# Create visualization of NaN patterns\n",
    "plt.figure(figsize=(15, 8))\n",
    "nan_matrix = combined_df[[col for col in combined_df.columns if col.endswith('_was_nan')]].astype(int)\n",
    "sns.heatmap(nan_matrix.corr(), cmap='coolwarm', center=0)\n",
    "plt.title('Correlation between NaN Patterns')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bc443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product combinations analysis\n",
    "def analyze_product_combinations(df):\n",
    "    \"\"\"Analyze which products are commonly held together\"\"\"\n",
    "    have_cols = [col for col in df.columns if col.startswith('Have_')]\n",
    "    product_combinations = df[have_cols].sum()\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df[have_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Product Combination Correlations')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return product_combinations\n",
    "\n",
    "print(\"Product ownership analysis:\")\n",
    "display(analyze_product_combinations(combined_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
